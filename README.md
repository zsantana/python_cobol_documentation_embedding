# ğŸ” COBOL/JCL Semantic Search

ğŸš€ Projeto para ingestÃ£o de documentaÃ§Ã£o COBOL/JCL, vetorizaÃ§Ã£o via OpenAI embeddings (text-embedding-3-small) e busca semÃ¢ntica com PostgreSQL + pgvector.

## ğŸ› ï¸ Setup

### ğŸ“¦ 1. Instalar DependÃªncias
```bash
pip install -r requirements.txt
```

### ğŸ—„ï¸ 2. Configurar PostgreSQL com pgvector
```sql
CREATE EXTENSION IF NOT EXISTS vector;
```
*(Crie a tabela `documentos_vetorizados` conforme instruÃ§Ãµes do script)*

### ğŸ”‘ 3. Configurar VariÃ¡vel de Ambiente
```bash
export OPENAI_API_KEY="sua_chave"
```

### ğŸ“ 4. Adicionar Documentos
Coloque seus arquivos `.md` na pasta `documentos/`

### âš¡ 5. Executar IngestÃ£o
```bash
python ingestao.py
```

### ğŸŒ 6. Executar Interface Streamlit
```bash
streamlit run app_streamlit.py
```

## ğŸ“Š MÃ©tricas de Similaridade

### 1ï¸âƒ£ DistÃ¢ncia Euclidiana (L2 norm)

ğŸ“ A distÃ¢ncia euclidiana mede o "espaÃ§o" entre dois vetores:

```
d(u,v) = âˆš(Î£(uáµ¢ - váµ¢)Â²)
```

âœ… **Regra**: Vetores mais prÃ³ximos â†’ menor distÃ¢ncia â†’ mais similares

ğŸ’» **No PostgreSQL + pgvector**:
```sql
embedding <=> %s::vector
```

â„¹ï¸ O operador `<=>` retorna a distÃ¢ncia Euclidiana (L2)

ğŸ”§ No `consulta.py`, jÃ¡ usamos `embedding <=> %s::vector` para ordenar por similaridade crescente.

### 2ï¸âƒ£ Similaridade do Cosseno (Cosine similarity)

ğŸ“ˆ Outra abordagem muito usada em NLP Ã© a similaridade do cosseno:

```
cosine_sim(u,v) = (uÂ·v) / (||u|| ||v||)
```

ğŸ“ **Valor**: entre -1 e 1, mais prÃ³ximo de 1 = vetores mais similares

ğŸ’» **Com pgvector**, vocÃª pode usar o operador `<#>`:
```sql
embedding <#> %s::vector
```

âš¡ **ObservaÃ§Ã£o**: No caso da OpenAI embeddings, a distÃ¢ncia euclidiana e a similaridade do cosseno sÃ£o praticamente equivalentes para rankings, porque os vetores jÃ¡ sÃ£o normalizados.

## ğŸ“‹ ComparaÃ§Ã£o das MÃ©tricas

| ğŸ“Š MÃ©trica | ğŸ”„ Ordem de similaridade | âš™ï¸ Quando usar |
|------------|-------------------------|----------------|
| ğŸ“ Euclidiana | menor â†’ mais similar | padrÃ£o, rÃ¡pido, compatÃ­vel com ivfflat |
| ğŸ“ˆ Cosseno | maior â†’ mais similar | mais intuitivo para NLP puro, funciona bem para vetores normalizados |

## ğŸ’¡ Dica Final

ğŸ¯ Como jÃ¡ estamos usando `<=>` com `ORDER BY embedding <=> %s::vector`, a busca jÃ¡ estÃ¡ funcionando como "proximal similarity".

ğŸ”„ Mas se vocÃª quiser **cosine similarity**, Ã© sÃ³ trocar `<=>` por `<#>` e ordenar `DESC`.